{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9bb655",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Review Chapters 15 of Raschka book  and submit one jupyter notebook as a tutorial to implement and train RNNs to predict the sentiment of IMDb movie reviews  (HW10a.ipynb). (25 points ) \n",
    "\n",
    "Your tutorial notebook should have the following sections and corresponding code examples: \n",
    "* Preparing the movie review data \n",
    "* Embedding layers for sentence encoding \n",
    "* Building and training an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498910f0",
   "metadata": {},
   "source": [
    "### Preparing the movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb02e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0877dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# step 1: load and create the datasets\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95eda3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 69023\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# step 2: find unique tokens\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc7d6b",
   "metadata": {},
   "source": [
    "```Counter``` documentation: https://docs.python.org/3/library/collections.html#collections.Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d28a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "# step 3: encode tokens into integers\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token(\"<pad>\", 0) # special token\n",
    "vocab.insert_token(\"<unk>\", 1) # special token\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f55132",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-A: define the functions for transformation\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
    "\n",
    "## Step 3-B: wrap the encode and transformation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), \n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6c41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a small batch\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee07a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide datasets into 3 dataloaders\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fb4fa",
   "metadata": {},
   "source": [
    "### Embedding layers for sentence encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30f210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1187, -0.5282,  0.7039],\n",
      "         [-0.8321, -0.4651,  0.3234],\n",
      "         [-0.3531,  0.9124,  0.3710],\n",
      "         [-0.3757,  0.7046, -0.7106]],\n",
      "\n",
      "        [[-0.3531,  0.9124,  0.3710],\n",
      "         [-0.1976,  0.5566,  0.0946],\n",
      "         [-0.8321, -0.4651,  0.3234],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3,\n",
    "    padding_idx=0)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd53033",
   "metadata": {},
   "source": [
    "### Building an RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bacaff",
   "metadata": {},
   "source": [
    "Using the nn.Module class, we can combine the embedding layer, the recurrent layers of the RNN, and the fully connected non-recurrent layers. For the recurrent layers, we can use any of the following implementations:\n",
    "* RNN: a regular RNN layer, that is, a fully connected recurrent layer\n",
    "* LSTM: a long short-term memory RNN, which is useful for capturing the long-term dependencies\n",
    "* GRU: a recurrent layer with a gated recurrent unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192fda4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1777],\n",
       "        [-0.2205],\n",
       "        [-0.2145],\n",
       "        [-0.0255],\n",
       "        [-0.3875]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN model wwith a non-recurrent fully connected output layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2,\n",
    "                          batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "        #                   batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "        #                    batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :] # we use the final hidden state\n",
    "                               # from the last hidden layer as\n",
    "                               # the input to the fully connected\n",
    "                               # layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = RNN(64, 32)\n",
    "print(model)\n",
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30e2d6",
   "metadata": {},
   "source": [
    "### Building an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7586cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size,\n",
    "                 fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac889b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim,\n",
    "            rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc23c84",
   "metadata": {},
   "source": [
    "Develop a ```train``` function to train the model on the given dataset for one epoch and return the classification accuracy and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6399697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a459b0",
   "metadata": {},
   "source": [
    "Develop an ```evaluate``` function to measure the model's performance on a given dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d17fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca3e61",
   "metadata": {},
   "source": [
    "Create loss function and optimizer objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c6eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55812ae",
   "metadata": {},
   "source": [
    "Train the model for 10 epochs and display training and valudation performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I am having some trouble running this code. It is taking a very\n",
    "long time to run and my laptop is running out of memory, however, \n",
    "when it runs, it should print out the accuracy for the training\n",
    "and validation set at each epoch.\n",
    "'''\n",
    "num_epochs = 10\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372758b7",
   "metadata": {},
   "source": [
    "Evaluate on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e174cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the evaluate function just like above\n",
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlms2023",
   "language": "python",
   "name": "mlms2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
